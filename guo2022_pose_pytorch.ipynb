{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# from keras.layers import BatchNormalization, Dense, Input, Conv1D, Add, ELU, Flatten, MaxPooling1D\n",
    "# from keras.layers import GlobalAveragePooling1D, Softmax, Concatenate, Reshape, Multiply, ReLU\n",
    "# from keras.optimizers import SGD\n",
    "# from keras import activations\n",
    "# from keras import Model\n",
    "# from keras.initializers import HeNormal\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# from keras.utils.vis_utils import plot_model\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryPrecision, BinaryRecall, BinaryF1Score, BinaryConfusionMatrix\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "from pytorch_symbolic import Input, SymbolicModel, useful_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# dataset\n",
    "class DAICDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        annotations_file,\n",
    "        sample_dir,\n",
    "        feature_type,\n",
    "        # transform=None,\n",
    "        target_transform=None,\n",
    "    ):\n",
    "        self.depression_labels = pd.read_csv(annotations_file)\n",
    "        self.sample_dir = sample_dir\n",
    "        self.feature_type = feature_type\n",
    "        self.transform = Compose([ToTensor()])\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.depression_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        participant_id = str(self.depression_labels.iloc[idx, 0])\n",
    "        individual = participant_id + \"/\" + participant_id + \"_CLNF_\" + self.feature_type + \".txt\"\n",
    "        participant_path = os.path.join(\n",
    "            self.sample_dir,\n",
    "            individual,\n",
    "        )\n",
    "        data = pd.read_csv(participant_path, sep=\",\")  # read_image(img_path)\n",
    "        data.columns = data.columns.str.replace(\" \", \"\")\n",
    "        data.drop(columns=[\"frame\", \"timestamp\", \"confidence\", \"success\"], inplace=True)\n",
    "        label = self.depression_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            data = self.transform(data.copy().loc[1000:5999].to_numpy(dtype=\"float32\").transpose())\n",
    "            # print(type(label))\n",
    "            # label = self.transform(label)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return data[0], label\n",
    "\n",
    "\n",
    "label_path = Path(\"original_daic/labels\")\n",
    "pose_train = DAICDataset(\n",
    "    annotations_file = label_path / \"train_split_Depression_AVEC2017.csv\",\n",
    "    sample_dir = \"original_daic/train\",\n",
    "    feature_type = \"pose\",\n",
    "    # transform=None,\n",
    "    target_transform=None,\n",
    ")\n",
    "pose_dev = DAICDataset(\n",
    "    annotations_file = label_path / \"dev_split_Depression_AVEC2017.csv\",\n",
    "    sample_dir = \"original_daic/dev\",\n",
    "    feature_type = \"pose\",\n",
    "    # transform=None,\n",
    "    target_transform=None,\n",
    ")\n",
    "# pose_test = DAICDataset(\n",
    "#     annotations_file = label_path / \"full_test_split.csv\",\n",
    "#     sample_dir = \"original_daic/test\",\n",
    "#     feature_type = \"pose\",\n",
    "#     # transform=None,\n",
    "#     target_transform=None,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader\n",
    "BATCH_SIZE = 1\n",
    "train_dataloader = DataLoader(pose_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_dataloader = DataLoader(pose_dev, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# test_dataloader = DataLoader(pose_test, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pose = Input(shape=[6, 5000])\n",
    "tdcn_dim_pose = [input_pose.shape[1],128,64,256,128,64] # used in Guo's paper\n",
    "# tdcn_dim_pose = [input_pose[0],128,128,128,128,128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diluted_conv_block(inputs, input_dim, feature_dim):\n",
    "    # with K.name_scope(block_name)\n",
    "    l1_p1 = nn.Conv1d(input_dim, feature_dim, kernel_size=3, padding=\"same\", dilation=1, groups=1, bias=True)(inputs)\n",
    "    l1_p2 = nn.Conv1d(input_dim, feature_dim, kernel_size=3, padding=\"same\", dilation=1, groups=1, bias=True)(inputs)\n",
    "    # l1_add = Add()([l1_p1, l1_p2])\n",
    "    l1_ELU = nn.ELU()(l1_p1 + l1_p2)\n",
    "    # second layer of the DCB\n",
    "    l2_p1 = nn.Conv1d(feature_dim, feature_dim, kernel_size=3, padding=\"same\", dilation=2, groups=1, bias=True)(l1_ELU)\n",
    "    l2_p2 = nn.Conv1d(feature_dim, feature_dim, kernel_size=3, padding=\"same\", dilation=2, groups=1, bias=True)(l1_ELU)\n",
    "    # l2_add = Add()([l2_p1, l2_p2])\n",
    "    l2_ELU = nn.ELU()(l2_p1 + l2_p2)\n",
    "    # third layer of the DCB\n",
    "    l3_p1 = nn.Conv1d(feature_dim, feature_dim, kernel_size=3, padding=\"same\", dilation=4, groups=1, bias=True)(l2_ELU)\n",
    "    l3_p2 = nn.Conv1d(feature_dim, feature_dim, kernel_size=3, padding=\"same\", dilation=4, groups=1, bias=True)(l2_ELU)\n",
    "    # l3_add = Add()([l3_p1, l3_p2])\n",
    "    l3_ELU = nn.ELU()(l3_p1 + l3_p2)\n",
    "\n",
    "    residual = nn.Conv1d(input_dim, feature_dim, kernel_size=1, padding=\"same\")(inputs)\n",
    "    # res_add = Add()([l3_ELU, residual])\n",
    "    # res_add = Add()([l1_ELU, residual])\n",
    "    # res_add = ELU()(res_add)\n",
    "    bn = nn.BatchNorm1d(num_features=feature_dim)(l3_ELU + residual)\n",
    "    return bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 312])\n",
      "torch.Size([1, 19968])\n"
     ]
    }
   ],
   "source": [
    "def time_diluted_conv_net(feature_dim, input_layer, pool_size, pool_stride):\n",
    "    dcb_1 = diluted_conv_block(input_layer, feature_dim[0], feature_dim[1])\n",
    "    mp_1 = nn.MaxPool1d(pool_size, stride=pool_stride)(dcb_1)\n",
    "    dcb_2 = diluted_conv_block(mp_1, feature_dim[1], feature_dim[2])\n",
    "    mp_2 = nn.MaxPool1d(pool_size, stride=pool_stride)(dcb_2)\n",
    "    dcb_3 = diluted_conv_block(mp_2, feature_dim[2], feature_dim[3])\n",
    "    mp_3 = nn.MaxPool1d(pool_size, stride=pool_stride)(dcb_3)\n",
    "    dcb_4 = diluted_conv_block(mp_3, feature_dim[3], feature_dim[4])\n",
    "    mp_4 = nn.MaxPool1d(pool_size, stride=pool_stride)(dcb_4)\n",
    "    dcb_5 = diluted_conv_block(mp_4, feature_dim[4], feature_dim[5])\n",
    "    return dcb_5\n",
    "\n",
    "tdcn_pose = time_diluted_conv_net(\n",
    "    feature_dim = tdcn_dim_pose, \n",
    "    input_layer = input_pose, \n",
    "    pool_size = 2, \n",
    "    pool_stride = 2,\n",
    "    )\n",
    "\n",
    "# concat = useful_layers.ConcatLayer([tdcn_pose])\n",
    "gap_layer = nn.AdaptiveMaxPool1d(1)(tdcn_pose)\n",
    "# print(gap_layer.shape)\n",
    "# linear_layer_1 = nn.Linear(gap_layer.shape[1], gap_layer.shape[1])(gap_layer)\n",
    "linear_layer_1 = nn.Linear(1, 1)(gap_layer)\n",
    "relu_layer = nn.ReLU()(linear_layer_1)\n",
    "# linear_layer_2 = nn.Linear(gap_layer.shape[1], gap_layer.shape[1])(relu_layer)\n",
    "linear_layer_2 = nn.Linear(1, 1)(relu_layer)\n",
    "sigmoid_layer = nn.Sigmoid()(linear_layer_2)\n",
    "reshape = sigmoid_layer\n",
    "for _ in range(0, tdcn_pose.shape[2]-1):\n",
    "    reshape = useful_layers.ConcatLayer(dim=2)(reshape, sigmoid_layer)\n",
    "# print(reshape.shape)\n",
    "# print(tdcn_pose.shape)\n",
    "print((tdcn_pose*reshape).shape)\n",
    "flatten = nn.Flatten()(tdcn_pose*reshape)\n",
    "print(flatten.shape)\n",
    "FC_l1 = nn.Linear(flatten.shape[1], 16)(flatten)(nn.ReLU())\n",
    "FC_l2 = nn.Linear(FC_l1.shape[1], 12)(FC_l1)(nn.ReLU())\n",
    "FC_l3 = nn.Linear(FC_l2.shape[1], 8)(FC_l2)(nn.ReLU())\n",
    "last_layer = nn.Linear(FC_l3.shape[1], 2)(FC_l3)(nn.Sigmoid())\n",
    "output = nn.Softmax(1)(last_layer)\n",
    "# output = nn.Linear(FC_l3.shape[1], 1)(FC_l3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "model = SymbolicModel(inputs=input_pose, outputs=output).to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=2e-5, momentum=0.9)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        # pred = model(X)[0]\n",
    "        loss = loss_fn(pred, y.long())\n",
    "        # loss = loss_fn(pred, y.float())\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dev(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred=model(X)\n",
    "            # pred = model(X)[0]\n",
    "            test_loss += loss_fn(pred, y.long()).item()\n",
    "            # test_loss += loss_fn(pred, y.float()).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            # correct += ((pred > 0.5) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model):\n",
    "    # size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    threshold = 0.5\n",
    "    accuracy = BinaryAccuracy(threshold=threshold)\n",
    "    precision = BinaryPrecision(threshold=threshold)\n",
    "    recall = BinaryRecall(threshold=threshold)\n",
    "    f1score = BinaryF1Score(threshold=threshold)\n",
    "    cm = BinaryConfusionMatrix(threshold=threshold)\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            preds.append(pred)\n",
    "            pred = torch.unsqueeze(torch.argmax(pred), 0)\n",
    "            # pred = model(X)[0]\n",
    "            accuracy.update(pred, y)\n",
    "            precision.update(pred, y)\n",
    "            recall.update(pred, y)\n",
    "            f1score.update(pred, y)\n",
    "            cm.update(pred, y)\n",
    "    f_acc = accuracy.compute()\n",
    "    f_pre = precision.compute()\n",
    "    f_rec = recall.compute()\n",
    "    f_f1s = f1score.compute()\n",
    "    f_cm = cm.compute()\n",
    "    return [f_acc,f_pre,f_rec,f_f1s,f_cm,preds]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.715810  [    1/  107]\n",
      "loss: 0.668932  [  101/  107]\n",
      "Test Error: \n",
      " Accuracy: 34.3%, Avg loss: 0.700787 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.716018  [    1/  107]\n",
      "loss: 0.716078  [  101/  107]\n",
      "Test Error: \n",
      " Accuracy: 34.3%, Avg loss: 0.700586 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.669173  [    1/  107]\n",
      "loss: 0.671814  [  101/  107]\n",
      "Test Error: \n",
      " Accuracy: 34.3%, Avg loss: 0.700582 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.715042  [    1/  107]\n",
      "loss: 0.715297  [  101/  107]\n",
      "Test Error: \n",
      " Accuracy: 34.3%, Avg loss: 0.700399 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.716617  [    1/  107]\n",
      "loss: 0.714244  [  101/  107]\n",
      "Test Error: \n",
      " Accuracy: 34.3%, Avg loss: 0.700295 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    dev(dev_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0.2979), tensor(0.2979), tensor(1.), tensor(0.4590), tensor([[ 0., 33.],\n",
      "        [ 0., 14.]]), [tensor([[0.4787, 0.5213]]), tensor([[0.4764, 0.5236]]), tensor([[0.4769, 0.5231]]), tensor([[0.4788, 0.5212]]), tensor([[0.4752, 0.5248]]), tensor([[0.4802, 0.5198]]), tensor([[0.4739, 0.5261]]), tensor([[0.4822, 0.5178]]), tensor([[0.4766, 0.5234]]), tensor([[0.4713, 0.5287]]), tensor([[0.4759, 0.5241]]), tensor([[0.4786, 0.5214]]), tensor([[0.4748, 0.5252]]), tensor([[0.4761, 0.5239]]), tensor([[0.4706, 0.5294]]), tensor([[0.4788, 0.5212]]), tensor([[0.4740, 0.5260]]), tensor([[0.4731, 0.5269]]), tensor([[0.4779, 0.5221]]), tensor([[0.4705, 0.5295]]), tensor([[0.4788, 0.5212]]), tensor([[0.4749, 0.5251]]), tensor([[0.4751, 0.5249]]), tensor([[0.4760, 0.5240]]), tensor([[0.4750, 0.5250]]), tensor([[0.4789, 0.5211]]), tensor([[0.4788, 0.5212]]), tensor([[0.4770, 0.5230]]), tensor([[0.4733, 0.5267]]), tensor([[0.4728, 0.5272]]), tensor([[0.4761, 0.5239]]), tensor([[0.4775, 0.5225]]), tensor([[0.4836, 0.5164]]), tensor([[0.4775, 0.5225]]), tensor([[0.4660, 0.5340]]), tensor([[0.4791, 0.5209]]), tensor([[0.4778, 0.5222]]), tensor([[0.4772, 0.5228]]), tensor([[0.4785, 0.5215]]), tensor([[0.4757, 0.5243]]), tensor([[0.4783, 0.5217]]), tensor([[0.4795, 0.5205]]), tensor([[0.4795, 0.5205]]), tensor([[0.4742, 0.5258]]), tensor([[0.4795, 0.5205]]), tensor([[0.4783, 0.5217]]), tensor([[0.4772, 0.5228]])]]\n"
     ]
    }
   ],
   "source": [
    "metrics = test(dev_dataloader, model)\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADD2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
