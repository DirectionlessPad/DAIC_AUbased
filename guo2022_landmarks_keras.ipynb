{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from keras.layers import BatchNormalization, Dense, Input, Conv1D, Add, ELU, Flatten, MaxPooling1D\n",
    "from keras.layers import GlobalAveragePooling1D, Softmax, Concatenate, Reshape, Multiply, ReLU\n",
    "from keras.optimizers import SGD\n",
    "from keras import activations\n",
    "from keras import Model\n",
    "from keras.initializers import HeNormal\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from load_dataset import load_original_daic, load_labels\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "daic_lm_train = load_original_daic(\"original_daic/train\", \"features\")\n",
    "daic_lm_dev = load_original_daic(\"original_daic/dev\", \"features\")\n",
    "daic_lm_test = load_original_daic(\"original_daic/test\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_path = Path(\"original_daic/labels\")\n",
    "loaded_labels = load_labels(label_path)\n",
    "y_train = np.array(loaded_labels[\"train\"])\n",
    "y_dev = np.array(loaded_labels[\"dev\"])\n",
    "y_test = np.array(loaded_labels[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_train = []\n",
    "lm_dev = []\n",
    "lm_test = []\n",
    "for subject_id, subject_df in daic_lm_train.items():\n",
    "    temp = subject_df.copy().loc[1000:5999].to_numpy(dtype=\"float32\")\n",
    "    lm_train.append(temp)\n",
    "for subject_id, subject_df in daic_lm_dev.items():\n",
    "    # print(subject_id)\n",
    "    temp = subject_df.copy().loc[1000:5999].to_numpy(dtype=\"float32\")\n",
    "    lm_dev.append(temp)\n",
    "for subject_id, subject_df in daic_lm_test.items():\n",
    "    # print(subject_id)\n",
    "    temp = subject_df.copy().loc[1000:5999].to_numpy(dtype=\"float32\")\n",
    "    lm_test.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples = lm_train + lm_dev + lm_test\n",
    "# all_samples = np.array(pose_train + pose_dev + pose_test, dtype=object)\n",
    "all_samples = np.concatenate((all_samples),)\n",
    "# all_samples.shape\n",
    "min_values = all_samples.min(axis=0)\n",
    "ptp_ranges = all_samples.ptp(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_train = np.array(lm_train)\n",
    "lm_dev = np.array(lm_dev)\n",
    "lm_test = np.array(lm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_lm_train = (lm_train - min_values)/ptp_ranges\n",
    "norm_lm_dev = (lm_dev - min_values)/ptp_ranges\n",
    "norm_lm_test = (lm_test - min_values)/ptp_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154\n"
     ]
    }
   ],
   "source": [
    "# depressed_only = norm_pose_train[train_depressed_idx]\n",
    "indices = range(norm_lm_train.shape[0])\n",
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "# oversample = RandomOverSampler(sampling_strategy=0.75)\n",
    "# print(norm_pose_train[:].shape)\n",
    "indices = np.array(indices).reshape(-1,1)\n",
    "ros_indices, over_train_y = oversample.fit_resample(indices, y_train)\n",
    "print(len(ros_indices))\n",
    "ros_norm_lm_train = np.squeeze(norm_lm_train[ros_indices])\n",
    "# print(ros_norm_pose_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lm = Input(shape=[5000, 136])\n",
    "tdcn_dim_lm = [256,256,128,64,64] # used in Guo's paper\n",
    "# tdcn_dim_lm = [256,256,256,256,256]\n",
    "bias_initializer = HeNormal(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One DCB\n",
    "# first layer of the DCB\n",
    "def diluted_conv_block(inputs, feature_dim):\n",
    "    # with K.name_scope(block_name)\n",
    "    l1_p1 = Conv1D(filters=feature_dim, kernel_size=3, padding=\"same\", dilation_rate=1, use_bias=True, bias_initializer=bias_initializer)(inputs)\n",
    "    l1_p2 = Conv1D(filters=feature_dim, kernel_size=3, padding=\"same\", dilation_rate=1, use_bias=True, bias_initializer=bias_initializer)(inputs)\n",
    "    l1_add = Add()([l1_p1, l1_p2])\n",
    "    l1_ELU = ELU()(l1_add)\n",
    "    # second layer of the DCB\n",
    "    l2_p1 = Conv1D(filters=feature_dim, kernel_size=5, padding=\"same\", dilation_rate=2, use_bias=True, bias_initializer=bias_initializer)(l1_ELU)\n",
    "    l2_p2 = Conv1D(filters=feature_dim, kernel_size=5, padding=\"same\", dilation_rate=2, use_bias=True, bias_initializer=bias_initializer)(l1_ELU)\n",
    "    l2_add = Add()([l2_p1, l2_p2])\n",
    "    l2_ELU = ELU()(l2_add)\n",
    "    # third layer of the DCB\n",
    "    l3_p1 = Conv1D(filters=feature_dim, kernel_size=9, padding=\"same\", dilation_rate=4, use_bias=True, bias_initializer=bias_initializer)(l2_ELU)\n",
    "    l3_p2 = Conv1D(filters=feature_dim, kernel_size=9, padding=\"same\", dilation_rate=4, use_bias=True, bias_initializer=bias_initializer)(l2_ELU)\n",
    "    l3_add = Add()([l3_p1, l3_p2])\n",
    "    l3_ELU = ELU()(l3_add)\n",
    "\n",
    "    residual = Conv1D(filters=feature_dim, kernel_size=1, padding=\"same\")(inputs)\n",
    "    res_add = Add()([l3_ELU, residual])\n",
    "    # res_add = Add()([l1_ELU, residual])\n",
    "    # res_add = ELU()(res_add)\n",
    "    bn = BatchNormalization()(res_add)\n",
    "    return bn\n",
    "    # return residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 312, 64)\n",
      "(None, 19968)\n"
     ]
    }
   ],
   "source": [
    "def time_diluted_conv_net(feature_dim, input_layer, pool_size, pool_stride):\n",
    "    dcb_1 = diluted_conv_block(input_layer, feature_dim[0])\n",
    "    mp_1 = MaxPooling1D(pool_size=pool_size, strides=pool_stride, padding='valid')(dcb_1)\n",
    "    dcb_2 = diluted_conv_block(mp_1, feature_dim[1])\n",
    "    mp_2 = MaxPooling1D(pool_size=pool_size, strides=pool_stride, padding='valid')(dcb_2)\n",
    "    dcb_3 = diluted_conv_block(mp_2, feature_dim[2])\n",
    "    mp_3 = MaxPooling1D(pool_size=pool_size, strides=pool_stride, padding='valid')(dcb_3)\n",
    "    dcb_4 = diluted_conv_block(mp_3, feature_dim[3])\n",
    "    mp_4 = MaxPooling1D(pool_size=pool_size, strides=pool_stride, padding='valid')(dcb_4)\n",
    "    dcb_5 = diluted_conv_block(mp_4, feature_dim[4])\n",
    "    return dcb_5\n",
    "    # return dcb_2\n",
    "\n",
    "# TDCN block for pose\n",
    "tdcn_lm = time_diluted_conv_net(\n",
    "    feature_dim = tdcn_dim_lm, \n",
    "    input_layer = input_lm, \n",
    "    pool_size = 2, \n",
    "    pool_stride = 2,\n",
    "    )\n",
    "\n",
    "# FWA block\n",
    "concat_layer = Concatenate()([tdcn_lm])\n",
    "# print(concat_layer.shape)\n",
    "# gap_layer = GlobalAveragePooling1D(data_format=\"channels_last\")(concat_layer)\n",
    "gap_layer = GlobalAveragePooling1D(data_format=\"channels_last\")(tdcn_lm)\n",
    "# print(gap_layer.shape)\n",
    "\n",
    "linear_layer_1 = Dense(gap_layer.shape[1])(gap_layer)\n",
    "# relu_layer = Dense(128, activation = \"relu\")(linear_layer_1)\n",
    "relu_layer = activations.relu(linear_layer_1)\n",
    "# relu_layer = ReLU()(linear_layer_1)\n",
    "linear_layer_2 = Dense(gap_layer.shape[1])(relu_layer)\n",
    "# sigmoid_layer = Dense(128, activation = \"sigmoid\")(linear_layer_2)\n",
    "sigmoid_layer = activations.sigmoid(linear_layer_2)\n",
    "# reshape_layer = Reshape((312, 128),)(sigmoid_layer)\n",
    "# print(reshape_layer.output_shape)\n",
    "# elementwise_product = Multiply()([concat_layer, reshape_layer])\n",
    "# elementwise_product = Multiply()([concat_layer, sigmoid_layer])\n",
    "elementwise_product = Multiply()([tdcn_lm, sigmoid_layer])\n",
    "print(elementwise_product.shape)\n",
    "# FC layer\n",
    "flatten = Flatten()(elementwise_product)\n",
    "print(flatten.shape)\n",
    "FC_l1 = Dense(16, activation=\"relu\")(flatten)\n",
    "FC_l2 = Dense(12, activation=\"relu\")(FC_l1)\n",
    "FC_l3 = Dense(8, activation=\"relu\")(FC_l2)\n",
    "last_layer = Dense(2, activation=\"sigmoid\")(FC_l3)\n",
    "output = Softmax()(last_layer)\n",
    "# output = Dense(1, activation = \"sigmoid\")(FC_l3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lm = Model(inputs=[input_lm], outputs=[output])\n",
    "opt = SGD(learning_rate = 2e-5, momentum = 0.9, nesterov=True)\n",
    "model_lm.compile(loss = \"binary_crossentropy\", optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "20/20 [==============================] - 60s 3s/step - loss: 0.6950 - accuracy: 0.5065 - val_loss: 0.6938 - val_accuracy: 0.3429\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 60s 3s/step - loss: 0.6951 - accuracy: 0.5000 - val_loss: 0.6953 - val_accuracy: 0.3429\n",
      "Epoch 3/10\n",
      " 8/20 [===========>..................] - ETA: 36s - loss: 0.6951 - accuracy: 0.4844"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# model_lm.fit(lm_train[:], y_train[:], validation_data=(lm_dev[:], y_dev[:]), epochs=5)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model_lm\u001b[39m.\u001b[39;49mfit(ros_norm_lm_train[:], over_train_y, validation_data\u001b[39m=\u001b[39;49m(norm_lm_dev[:], y_dev[:]), epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\u1737136\\Anaconda3\\envs\\attempt2\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\u1737136\\Anaconda3\\envs\\attempt2\\lib\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\u1737136\\Anaconda3\\envs\\attempt2\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\u1737136\\Anaconda3\\envs\\attempt2\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\u1737136\\Anaconda3\\envs\\attempt2\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\u1737136\\Anaconda3\\envs\\attempt2\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\u1737136\\Anaconda3\\envs\\attempt2\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\u1737136\\Anaconda3\\envs\\attempt2\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\u1737136\\Anaconda3\\envs\\attempt2\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model_lm.fit(lm_train[:], y_train[:], validation_data=(lm_dev[:], y_dev[:]), epochs=5)\n",
    "model_lm.fit(ros_norm_lm_train[:], over_train_y, validation_data=(norm_lm_dev[:], y_dev[:]), epochs=10, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict\n",
    "x = lm_test\n",
    "y = y_test\n",
    "# x = x_dev_bad\n",
    "# y = y_dev_bad\n",
    "y_lm_prediction = model_lm.predict(x)\n",
    "y_lm_prediction = np.argmax(y_lm_prediction, axis = 1)\n",
    "# print(sum(y_pose_prediction)/len(y_pose_prediction))\n",
    "print(y_lm_prediction)\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(y, y_lm_prediction, normalize='pred')\n",
    "print(result)\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix = result, display_labels = [\"Non-Depressed\", \"Depressed\"])\n",
    "cm_display.plot()\n",
    "plt.show()\n",
    "tn_pose = result[0][0]\n",
    "tp_pose = result[1][1]\n",
    "fn_pose = result[1][0]\n",
    "fp_pose = result[0][1]\n",
    "accuracy = (tp_pose + tn_pose)/(tp_pose + tn_pose + fp_pose + fn_pose)\n",
    "recall = tp_pose / (tp_pose + fn_pose)\n",
    "precision = tp_pose / (tp_pose + fp_pose)\n",
    "f1_score = 2 * precision * recall / (precision + recall)\n",
    "print(\"Accuracy = \" + str(accuracy))\n",
    "print(\"Recall = \" + str(recall))\n",
    "print(\"Precision = \" + str(precision))\n",
    "print(\"F1-score = \" + str(f1_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attempt2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
